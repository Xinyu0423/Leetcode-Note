# Bert/XLM-R Interview Questions

## Bert
1. 整体架构
   1. 基础架构-tranformer的encoder
      1. Encode可以分为3个部分，输入部分，注意力机制，前馈神经网络
      2. Bert使用多个encoder堆叠在一起，bert base使用12层，bert launch使用24层encoder
      3. 6个encoder堆叠形成编码端，6个decoder堆叠形成解码端
      4. word embedding使用随机初始化或者word_to_vertex
      5. position encoding中使用token embedding+segement emb+position emb
   2. BERT 利用了Transformer 的encoder 部分。 Transformer 是一种注意力机制，可以学习文本中单词之间的上下文关系的。 Transformer 的原型包括两个独立的机制，一个encoder 负责接收文本作为输入，一个decoder 负责预测任务的结果。
2. Bert输入部分
   1. NSP(next sentence prediction)二分类任务，处理两个句子之间的关系
   2. Token Embedding对所有词汇，包括特殊词汇做embedding，包括随机初始化
   3. Segment Embedding区分两个/多个句子
   4. Position Embedding使用随机初始化后由模型去学习出来
   5. Transform中使用的是正余弦函数
3. 如何做预训练: MLM(mask language model)+NSP
   1. bert在预训练时使用的是大量的无标注的语料（无监督模型）
   2. 对于无监督，有两种目标函数
      1. AR(autoregressive)
         1. 自回归模型，只能考虑单侧的信息，比如GPT
      2. AE(Autoencoding)
         1. 自编码模型：从损坏的输入数据中预测重建原始数据，Bert使用的就是AE模型
         2. Mask的概率问题
            1. 随机mast15%的单词
               1. 10%替换成其他
               2. 10%原封不动
               3. 80%替换成mask
   3. NSP任务
      1. NSP训练样本
         1. 从训练语料库中抽取两个连续的段落作为正样本
         2. 从不同的文档中随机创建一对段位作为负样本
         3. 缺点：主题预测和连贯性预测合并成单项任务
4. 如何提升Bert下游任务的表现
   1. 在大量通用语料中训练一个language model（一般用已经训练好的）
   2. 在相同领域继续训练LM(Domain transfer)
      1. 使用动态mask：每次epoch去训练的时候使用mask，而不是一直使用同一个
   3. 在任务相关的小数据上继续训练LM（task transfer）
   4. 在任务相关数据上做具体任务（fine-tune）
   5. Batch size：16，32
   6. Learning rate：尽量小一点避免灾难性遗忘
   7. Number of epochs：3，4
5. 脱敏数据中如何使用Bert
   1. 如果数据很大，可以直接训练一个新的bert
   2. 数据很小，会导致欠拟合
6. Regular Bert的优缺点
   1. 优点：1）被随机选择15%的词当中以10%的概率用任意词替换去预测正确的词，相当于文本纠错任务，为BERT模型赋予了一定的文本纠错能力；2）被随机选择15%的词当中以10%的概率保持不变，缓解了finetune时候与预训练时候输入不匹配的问题（预训练时候输入句子当中有mask，而finetune时候输入是完整无缺的句子，即为输入不匹配问题）。
   2. 缺点：针对有两个及两个以上连续字组成的词，随机mask字割裂了连续字之间的相关性，使模型不太容易学习到词的语义信息。主要针对这一短板，因此google此后发表了BERT-WWM，国内的哈工大联合讯飞发表了中文版的BERT-WWM。





## XLM-R
1. 三种语言模型
   1. CLM(Casual Language Modeling)
      1. CLM的做法是直接使用Transformer来预测下一个词，是传统的语言模型的做法。
   2. MLM(Masked Language Modeling)
      1. - MLM的做法和BERT基本一致：对于输入的语句，按一定概率（15%）遮住一些词，在模型的末尾预测这些被遮住的词。在这里，Facebook和Google的BERT做法的差别是，他们使用了任意数量的句子拼接，并取256个词阶段，而非BERT原本使用的两个句子。类似地，为了平衡高频词低频词之间的差异，在遮住词的时候采用了与上文所述的重采样类似的方法。
   3. TLM(Translation Language Modeling)
      1. 在TLM中，输入的是两个意思相同语言不同的句子（比如“I ate an apple”和“我吃了一个苹果”），用句子分隔符拼接起来，整体作为模型的输入。然后再按照一定概率遮住一些词，再去预测这些词。MLM的训练方法，可以让模型利用其上下文信息预测被遮住的词。而TLM则是让模型不仅能利用这句话本身的上下文信息，而且同时能利用不同语言的翻译的信息，来预测被遮住的词。这样，模型就可以将跨语言信息编码到表征向量中。在XLM中，TLM和MLM交替训练，其中MLM的部分每次输入的一个batch内只包含一种语言。
      2. 除此之外，在XLM模型中，对于原来BERT中使用的Transformer也进行了改动，以更好地适用在跨语言场景下。在表示词相对句子位置的Positional Embedding中，TLM对拼接后的句子进行了位置重置，也就是接在后面的翻译句子的词下标重新从0开始计数。另外，在Positional Embedding的基础上加入了Language Embedding，用来区分不同语言的输入。
   4. 其中最有成效的XLM是MLM与TLM的结合。
   5. 模型细节：
      1. 在模型的细节方面，XLM模型采用了具有1024 hidden units, 8-head的多层（文本分类12层，机器翻译6层）Transformer以及GELU激活层，词汇表大小是95k。同时XLM模型使用了16位浮点数压缩模型空间，加快训练速度。
   6. 预训练数据集:
      1. 在预训练数据集上，Facebook使用了WikiExtractor得到的Wikipedia语料作为CLM和MLM的训练预料。对于TLM，Facebook使用了与MultiUN/IIT Bombay corpus/EUBookshop/OpenSubtitles等与英语有对照数据的数据集。其中，中文、日文和泰文使用了相对应的Tokenizer进行了切词。其他语言则统一使用Moses。
   7. 预训练效果
      1. 在预训练效果上，使用平行语料（意思相同语言不同的句子对），可以降低Perplexity指数。下图中，因为Nepali和Hindi之间字母表上有很大重叠，语言关系比较近，而跟英语之间则关系不大，所以引入英语对于Nepali带来的提升没有引入Hindi带来的提升多。
   8. 跨语言文本分类
      1. 对于跨语言文本分类，论文采用模型的最后一层的第一个输出向量作为后续线性分类器的输入，在英语的训练预料上进行拟合，然后在所有15个语言上做测试。使用的数据集是XNLI(Cross-Lingual Natural Language Inference)，其中Natural Language Inference的任务是判断两个来自相同语言的句子之间是否有Entailment, Contradict或者Natural的关系。XNLI数据集包含近40万条英语训练预料和上千条测试数据。
   9. 语料采样公式
      1.  q：某语料被采样的概率，q1=0.3，q2=0.2，q3=0.5
      2.  n：代表句子的数量
      3.  p：代表语料被选择的概率


## Macro/Micro Average/Weight/
1. Macro Average
   1. 对每个类别的 精准、召回和F1 加和求平均。
   2. 精准 macro avg=(P_no+P_yes)/2
2. Micro Average
   1. 不区分样本类别，计算整体的 精准、召回和F1
   2. 精准 macro avg=(P_nosupport_no+P_yessupport_yes)/(support_no+support_yes)
3. Weight Average:
   1. 是对宏平均的一种改进，考虑了每个类别样本数量在总样本中占比
   2. 精准 weighted avg =P_no*（support_no/support_all）+ P_yes*（support_yes/support_all
4. Support
   1. 支持度，即数据中实际类别个数。


## Machine Learning
1. 朴素贝叶斯
   1. 公式: P(B|A)=P(A|B)P(B)/P(A)
   2. 朴素贝叶斯采用 属性条件独立性 的假设。
   3. 优点:计算复杂度小，时间空间开销小
   4. 缺点：前提假设限制性太强，分类任务中的各个特征很少满足。
2. 决策树(Reference:https://blog.csdn.net/Datawhale/article/details/102878758)
   1. 决策树算法采用树形结构，使用层层推理来实现最终的分类。决策树由下面几种元素构成：
      1. 根节点：包含样本的全集
      2. 内部节点：对应特征属性测试
      3. 叶节点：代表决策的结果
   2. 预测时，在树的内部节点处用某一属性值进行判断，根据判断结果决定进入哪个分支节点，直到到达叶节点处，得到分类结果。
   3. 这是一种基于 if-then-else 规则的有监督学习算法，决策树的这些规则通过训练得到，而不是人工制定的。决策树是最简单的机器学习算法，它易于实现，可解释性强，完全符合人类的直观思维，有着广泛的应用。
   4. 1.决策树和条件概率分布的关系？
      1. 决策树可以表示成给定条件下类的条件概率分布. 决策树中的每一条路径都对应是划分的一个条件概率分布. 每一个叶子节点都是通过多个条件之后的划分空间，在叶子节点中计算每个类的条件概率，必然会倾向于某一个类，即这个类的概率最大.
   5. 决策树怎么剪枝？
      1. 一般算法在构造决策树的都是尽可能的细分，直到数据不可划分才会到达叶子节点，停止划分. 因为给训练数据巨大的信任，这种形式形式很容易造成过拟合，为了防止过拟合需要进行决策树剪枝. 一般分为预剪枝和后剪枝，预剪枝是在决策树的构建过程中加入限制，比如控制叶子节点最少的样本个数，提前停止. 后剪枝是在决策树构建完成之后，根据加上正则项的结构风险最小化自下向上进行的剪枝操作. 剪枝的目的就是防止过拟合，是模型在测试数据上变现良好，更加鲁棒.
   6. 决策树的优点？
      1. 优点: 决策树模型可读性好，具有描述性，有助于人工分析；效率高，决策树只需要一次性构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。
      2. 缺点: 对中间值的缺失敏感；可能产生过度匹配的问题，即过拟合。
3. 随机森林(Reference:https://blog.csdn.net/Heitao5200/article/details/103758643)
   1. 多次随机取样，多次随机取属性，选取最优分割点，构建多个(CART)分类器，投票表决
   2. 随机森林的随机性体现在哪里？
      1. 多次有放回的随机取样，多次随机取属性
   3. 随机森林为什么不容易过拟合？
      1. 随机森林中的每一颗树都是过拟合的，拟合到非常小的细节上
      2. 随机森林通过引入随机性，使每一颗树拟合的细节不同
      3. 所有树组合在一起，过拟合的部分就会自动被消除掉。
   4. 随机森林的优缺点
      1. 优点
         1. 训练可以高度并行化，对于大数据时代的大样本训练速度有优势。个人觉得这是的最主要的优点。
         2. 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。
         3. 在训练后，可以给出各个特征对于输出的重要性
         4. 由于采用了随机采样，训练出的模型的方差小，泛化能力强。
         5. 相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。
         6. 对部分特征缺失不敏感。
      2. 缺点
         1. 在某些噪音比较大的样本集上，RF模型容易陷入过拟合。
         2. 取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。
4. 逻辑回归(Reference：https://zhuanlan.zhihu.com/p/34670728)
   1. Logistic 回归的本质是：假设数据服从这个分布，然后使用极大似然估计做参数的估计，划分出决策边界(decision boundary).
   2. 怎么防止过拟合？
      1. 通过正则化方法。正则化方法是指在进行目标函数或代价函数优化时，在目标函数或代价函数后面加上一个正则项，一般有L1正则与L2正则等。
   3. 为什么正则化可以防止过拟合？
      1. 过拟合表现在训练数据上的误差非常小，而在测试数据上误差反而增大。其原因一般是模型过于复杂，过分得去拟合数据的噪声。正则化则是对模型参数添加先验，使得模型复杂度较小，对于噪声扰动相对较小。
   4. L1正则和L2正则有什么区别？
      1. L2与L1的区别在于，L1正则是拉普拉斯先验，而L2正则则是高斯先验。L1可以产生稀疏解,可以让一部分特征的系数缩小到0，从而间接实现特征选择。所以L1适用于特征之间有关联的情况。L2让所有特征的系数都缩小，但是不会减为0，它会使优化求解稳定快速。所以L2适用于特征之间没有关联的情况
   5. 逻辑回归为什么一般性能差？
      1. LR是线性的，不能得到非线性关系，实际问题并不完全能用线性关系就能拟合。
   6. 如何用LR解决非线性问题？
      1. 将特征离散成高维的01特征可以解决分类模型的非线性问题
   7. 优缺点
      1. 优点：
         1. （模型）模型清晰，背后的概率推导经得住推敲。
         2. （输出）输出值自然地落在0到1之间，并且有概率意义（逻辑回归的输出是概率么？https://www.jianshu.com/p/a8d6b40da0cf）。
         3. （参数）参数代表每个特征对输出的影响，可解释性强。
         4. （简单高效）实施简单，非常高效（计算量小、存储占用低），可以在大数据场景中使用。
         5. （可扩展）可以使用online learning的方式更新轻松更新参数，不需要重新训练整个模型。
         6. （过拟合）解决过拟合的方法很多，如L1、L2正则化。
         7. （多重共线性）L2正则化就可以解决多重共线性问题。
      2. 缺点：
         1. 缺点：
            1. （特征相关情况）因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。
            2. （特征空间）特征空间很大时，性能不好。
            3. （精度）容易欠拟合，精度不高。
5. SVM(Reference:https://zhuanlan.zhihu.com/p/43827793)
   1. SVM 是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。
      1. 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机.
      2. 当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；
      3. 当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。
   2.  SVM 为什么采用间隔最大化
       1.  当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。可以借此机会阐述一下几何间隔以及函数间隔的关系。
   3. 为什么 SVM 要引入核函数
      1. 当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。而引入这样的映射后，所要求解的对偶问题的求解中，无需求解真正的映射函数，而只需要知道其核函数。核函数的定义：K(x,y)=<ϕ(x),ϕ(y)>，即在特征空间的内积等于它们在原始样本空间中通过核函数 K 计算的结果。一方面数据变成了高维空间中线性可分的数据，另一方面不需要求解具体的映射函数，只需要给定具体的核函数即可，这样使得求解的难度大大降低。
   4. 为什么SVM对缺失数据敏感
      1. 这里说的缺失数据是指缺失某些特征数据，向量数据不完整。SVM 没有处理缺失值的策略。而 SVM 希望样本在特征空间中线性可分，所以特征空间的好坏对SVM的性能很重要。缺失特征数据将影响训练结果的好坏。
   5. 优缺点：
      1. 优点：
         1. 解决高维特征的分类问题和回归问题很有效,在特征维度大于样本数时依然有很好的效果。
         2. 仅仅使用一部分支持向量来做超平面的决策，无需依赖全部数据。
         3. 有大量的核函数可以使用，从而可以很灵活的来解决各种非线性的分类回归问题。
         4. 样本量不是海量数据的时候，分类准确率高，泛化能力强。
      2. 缺点：
         1. 如果特征维度远远大于样本数，则SVM表现一般。
         2. SVM在样本量非常大，核函数映射维度非常高时，计算量过大，不太适合使用。
         3. 非线性问题的核函数的选择没有通用标准，难以选择一个合适的核函数。
         4. SVM对缺失数据敏感。

## Accuracy/Precision/Recall/F1
1. Accuracy:
   1. Acc=(TP+TN)/(TP+TN+FP+FN)
   2. 注：准确率是我们最常见的评价指标，而且很容易理解，就是被分对的样本数除以所有的样本数，通常来说，正确率越高，分类器越好。
准确率确实是一个很好很直观的评价指标，但是有时候准确率高并不能代表一个算法就好。比如某个地区某天地震的预测，假设我们有一堆的特征作为地震分类的属性，类别只有两个：0：不发生地震、1：发生地震。一个不加思考的分类器，对每一个测试用例都将类别划分为0，那那么它就可能达到99%的准确率，但真的地震来临时，这个分类器毫无察觉，这个分类带来的损失是巨大的。为什么99%的准确率的分类器却不是我们想要的，因为这里数据分布不均衡，类别1的数据太少，完全错分类别1依然可以达到很高的准确率却忽视了我们关注的东西。再举个例子说明下。在正负样本不平衡的情况下，准确率这个评价指标有很大的缺陷。比如在互联网广告里面，点击的数量是很少的，一般只有千分之几，如果用acc，即使全部预测成负类（不点击）acc也有 99% 以上，没有意义。因此，单纯靠准确率来评价一个算法模型是远远不够科学全面的。
2. 精确率
   1. P=TP/TP+FP
   2. 表示被分为正例的示例中实际为正例的比例。
3. 召回率
   1. recall=TP/(TP+FN)=TP/P=sensitive
   2. 召回率是覆盖面的度量，度量有多个正例被分为正例
   3. 可以看到召回率与灵敏度是一样的。
4. F1
   1. F1=(2*P*R)/(P+R)
   2. P和R指标有时候会出现的矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure（又称为F-Score）。
   3. F1综合了P和R的结果，当F1较高时则能说明试验方法比较有效。
   
## 高频面试题
1. 解决过拟合（overfitting的方法）
   1. early stopping、数据集扩增（Data augmentation）、正则化（Regularization）、Dropout，BN
2. 解决欠拟合（underfitting的方法）
   1. 增加新特征，考虑加入特征组合，高次特征来增大假设空间
   2. 减少正则化参数。
   3. 使用非线性模型:比如核SVM，决策树，深度学习等模型
   4. 容量低的模型很难拟合训练集，使用集成学习方法，


### 面试问题
1. python的列表和元组的区别
   1. 元组和列表最大的区别就是，列表中的元素可以进行任意修改。而元组中的元素无法修改。
2. batchNorm的区别
   1. BatchNorm： 对一个batch-size样本内的每个特征做归一化
   2. LayerNorm： 针对每条样本，对每条样本的所有特征做归一化
   3. 简单举例：
      1. 假设现在有个二维矩阵：行代表batch-size， 列表示样本特征
      2. BatchNorm就是对这个二维矩阵中每一列的特征做归一化，也就是竖着做归一化
      3. LayerNorm就是对这个二维矩阵中每一行数据做归一化
   4. 相同点： 都是在深度学习中让当前层的参数稳定下来，避免梯度消失或者梯度爆炸，方便后面的继续学习
   5. 不同点：
      1. 如果你的特征依赖不同样本的统计参数，那BatchNorm更有效， 因为它不考虑不同特征之间的大小关系，但是保留不同样本间的大小关系
      2. Nlp领域适合用LayerNorm， CV适合BatchNorm，
      3. 对于Nlp来说，它不考虑不同样本间的大小关系，保留样本内不同特征之间的大小关系
3. BN、LN、IN、GN的区别和应用场景。
   1. Batch Normalization（BN）
      1. 解决Internal Covariate Shift问题：训练深层网络时，层内神经元间、层间神经元间激活值的量级差别较大，不满足iit独立同分布时，模型不稳定不容易收敛（直观来看解决方案要么自适应地调节每一层甚至每一个神经元的学习率，要么把神经元激活值规范化一下）。
      2. 缓解过拟合：深层网络容易过拟合，有时候dropout可能也解决不了。
      3. BN的缺点
         1. BN操作的效果受batchsize影响很大，如果batchsize较小，每次训练计算的均值方差不具有代表性且不稳定，甚至使模型效果恶化。
         2. BN很难用在RNN这种序列模型中，且效果不好
         3. 这一点算是BN的特点不能算是其缺点：训练和测试的BN参数是不同的
   2. Layer Normalization（LN）
      1. 层规范化LayerNormalization，LN是对当前隐藏层整层的神经元进行规范化
      2. LN VS BN：
         1. 对于[B,C,W,H]这样的训练数据而言，BN是在B,W,H维度求均值方差进行规范化，而LN是对C,W,H维度求均值方差进行规范化（当前层一共会求batchsize个均值和方差，每个batchsize分别规范化）
         2. 这样LN就与batchsize无关了，小的batchsize也可以进行归一化训练，LN也可以很轻松地用到RNN中。
         3. 总结
            1. LN与batchsize无关，在小batchsize上效果可能会比BN好，但是大batchsize的话还是BN效果好。
            2. LN可以轻松用到RNN上，且效果还不错
   3. Instance Normalization（IN）
      1. BN注重对batchsize数据归一化，但是在图像风格化任务中，生成的风格结果主要依赖于某个图像实例，所以对整个batchsize数据进行归一化是不合适的，因而提出了IN只对HW维度进行归一化。
      2. 在图像风格化任务中，更合适使用IN来做规范化。
      3. IN只对W,H维度求均值方差进行归一化。
   4. Group Normalization（GN）
      1. BN依赖大batchsize，LN虽然不依赖batchsize，但是在CNN中直接对当前层所有通道数据进行规范化也不太好。
      2. GN先对通道进行分组，每个组内的所有C_i,W,H维度求均值方差进行规范化，也与batchsize无关。
4. 什么是局部最小值？
   1. 梯度为零。
   2. 二阶导数大于0
5. 为什么模型越复杂，越不容易陷入局部最小值?
   1. 比如加入动量的梯度下降，在梯度下降后期陷入局部最小值的时候，有可能借助之前相同的动量越出局部最小值。
6. 小样本该怎么训练？
   1. 数据增强
   2. FocalLoss


## 改进
1. 加一些如何解决问题的途径
2. 增加bert基本的运行图
3. 为什么DNN
   1. 传统机器学习的解释性比较好
   2. 项目更侧重embedding，如果DNN效果太好，可能无法比较出差异
4. 说一下调参的方式，L1正则L2正则
5. 主要了解一下分类器的原理


## 流程
1. Project Goal:
   1. 介绍一下为什么使用这两种模型(都是sota)
   2. 这两种模型去做什么
   3. 介绍一下数据
   4. 在完成所有内容后差异化的原因
2. Bert-wwn
   1. 介绍wwm
   2. 为什么要用wwm，解决了MLM的什么问题
   3. 介绍一下bert的架构(输入端3种embedding)
   4. 举例子说明wwm的好处
3. XLM-R
   1. 介绍XLM-R模型和Bert的主要区别(有监督embedding)
   2. 介绍比bert多一层，具体有什么作用(语意获取)
   3. 减少了语种选择的bias，通过公式
   4. 控制变量，使用XLM-R没有使用XLM。
4. Result1
   1. 使用了precision，recall和F1来检测accuracy，并查看macro accuracy和weighted accuracy
   2. 混淆矩阵查看有没有过拟合/欠拟合，查看预测错误的情况
5. Rusult2
   1. 不论是bert还是XLM-R中都是Random Forest和logistic Regression结果更好
   2. Bert在接近的数据中效果更好，而且准确率也稍微高一点
6. Technique used：
   1. 混淆矩阵
   2. 多种分类器模型，random forest时间最长
7. Conclusion：
   1. Bert效果更好。
   2. XLMR更适合在多语言场景下的word embedding，但文本主要以中文为主，限制了它的发挥